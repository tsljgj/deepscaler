+ module load cuda-12.1
+ local _mlredir=1
+ '[' -n '' ']'
+ case " $@ " in
+ '[' 1 -eq 0 ']'
+ _module_raw load cuda-12.1
+ source activate deepscaler
/var/spool/slurmd/job4224854/slurm_script: line 17: activate: No such file or directory
+ export LD_LIBRARY_PATH=/home/mingkaid/miniforge3/envs/deepscaler/lib/python3.10/site-packages/nvidia/cusparse/lib/libcusparse.so.12
+ LD_LIBRARY_PATH=/home/mingkaid/miniforge3/envs/deepscaler/lib/python3.10/site-packages/nvidia/cusparse/lib/libcusparse.so.12
+ export VLLM_ATTENTION_BACKEND=XFORMERS
+ VLLM_ATTENTION_BACKEND=XFORMERS
+ nvcc --version
+ python3 -c 'import torch; print(torch.cuda.is_available(), torch.version.cuda)'
+ MODEL_PATH=/data/user_data/mingkaid/models/Qwen2.5-3B
+ python3 -m verl.trainer.main_ppo algorithm.adv_estimator=grpo data.train_files=/home/mingkaid/deepscaler/data/train.parquet data.val_files=/home/mingkaid/deepscaler/data/aime.parquet data.train_batch_size=128 data.val_batch_size=512 data.max_prompt_length=1024 data.max_response_length=8192 actor_rollout_ref.model.path=/data/user_data/mingkaid/models/Qwen2.5-3B actor_rollout_ref.actor.optim.lr=1e-6 actor_rollout_ref.model.use_remove_padding=True actor_rollout_ref.actor.ppo_mini_batch_size=64 actor_rollout_ref.actor.use_dynamic_bsz=True actor_rollout_ref.actor.ppo_max_token_len_per_gpu=32768 actor_rollout_ref.actor.use_kl_loss=True actor_rollout_ref.actor.kl_loss_coef=0.001 actor_rollout_ref.actor.kl_loss_type=low_var_kl actor_rollout_ref.actor.ulysses_sequence_parallel_size=1 actor_rollout_ref.model.enable_gradient_checkpointing=True actor_rollout_ref.actor.fsdp_config.param_offload=False actor_rollout_ref.actor.fsdp_config.grad_offload=False actor_rollout_ref.actor.fsdp_config.optimizer_offload=False actor_rollout_ref.rollout.tensor_model_parallel_size=1 actor_rollout_ref.rollout.name=vllm actor_rollout_ref.rollout.temperature=0.6 actor_rollout_ref.rollout.val_temperature=0.6 actor_rollout_ref.rollout.gpu_memory_utilization=0.85 actor_rollout_ref.rollout.n=8 actor_rollout_ref.rollout.n_val=8 actor_rollout_ref.ref.fsdp_config.param_offload=True algorithm.kl_ctrl.kl_coef=0.001 trainer.critic_warmup=0 'trainer.logger=[console,wandb]' trainer.project_name=deepscaler trainer.experiment_name=deepscaler-1.5b-8k +trainer.val_before_train=True trainer.n_gpus_per_node=8 trainer.nnodes=1 trainer.save_freq=20 trainer.test_freq=20 trainer.default_hdfs_dir=null trainer.total_epochs=30
2025-02-15 23:44:26,279	INFO worker.py:1832 -- Started a local Ray instance. View the dashboard at [1m[32m127.0.0.1:8265 [39m[22m
[36m(main_task pid=3419303)[0m /home/mingkaid/miniforge3/envs/deepscaler/lib/python3.10/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:
[36m(main_task pid=3419303)[0m No module named 'vllm._version'
[36m(main_task pid=3419303)[0m   from vllm.version import __version__ as VLLM_VERSION
Error executing job with overrides: ['algorithm.adv_estimator=grpo', 'data.train_files=/home/mingkaid/deepscaler/data/train.parquet', 'data.val_files=/home/mingkaid/deepscaler/data/aime.parquet', 'data.train_batch_size=128', 'data.val_batch_size=512', 'data.max_prompt_length=1024', 'data.max_response_length=8192', 'actor_rollout_ref.model.path=/data/user_data/mingkaid/models/Qwen2.5-3B', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.ppo_mini_batch_size=64', 'actor_rollout_ref.actor.use_dynamic_bsz=True', 'actor_rollout_ref.actor.ppo_max_token_len_per_gpu=32768', 'actor_rollout_ref.actor.use_kl_loss=True', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.kl_loss_type=low_var_kl', 'actor_rollout_ref.actor.ulysses_sequence_parallel_size=1', 'actor_rollout_ref.model.enable_gradient_checkpointing=True', 'actor_rollout_ref.actor.fsdp_config.param_offload=False', 'actor_rollout_ref.actor.fsdp_config.grad_offload=False', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=False', 'actor_rollout_ref.rollout.tensor_model_parallel_size=1', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.temperature=0.6', 'actor_rollout_ref.rollout.val_temperature=0.6', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.85', 'actor_rollout_ref.rollout.n=8', 'actor_rollout_ref.rollout.n_val=8', 'actor_rollout_ref.ref.fsdp_config.param_offload=True', 'algorithm.kl_ctrl.kl_coef=0.001', 'trainer.critic_warmup=0', 'trainer.logger=[console,wandb]', 'trainer.project_name=deepscaler', 'trainer.experiment_name=deepscaler-1.5b-8k', '+trainer.val_before_train=True', 'trainer.n_gpus_per_node=8', 'trainer.nnodes=1', 'trainer.save_freq=20', 'trainer.test_freq=20', 'trainer.default_hdfs_dir=null', 'trainer.total_epochs=30']
Traceback (most recent call last):
  File "/data/user_data/mingkaid/deepscaler/verl/verl/trainer/main_ppo.py", line 114, in main
    ray.get(main_task.remote(config))
  File "/home/mingkaid/miniforge3/envs/deepscaler/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/mingkaid/miniforge3/envs/deepscaler/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/mingkaid/miniforge3/envs/deepscaler/lib/python3.10/site-packages/ray/_private/worker.py", line 2772, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/mingkaid/miniforge3/envs/deepscaler/lib/python3.10/site-packages/ray/_private/worker.py", line 919, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(FileNotFoundError): [36mray::main_task()[39m (pid=3419303, ip=10.1.1.55)
  File "/data/user_data/mingkaid/deepscaler/verl/verl/trainer/main_ppo.py", line 192, in main_task
    trainer = RayPPOTrainer(config=config,
  File "/data/user_data/mingkaid/deepscaler/verl/verl/trainer/ppo/ray_trainer.py", line 349, in __init__
    self._create_dataloader()
  File "/data/user_data/mingkaid/deepscaler/verl/verl/trainer/ppo/ray_trainer.py", line 355, in _create_dataloader
    self.train_dataset = RLHFDataset(parquet_files=self.config.data.train_files,
  File "/data/user_data/mingkaid/deepscaler/verl/verl/utils/dataset/rl_dataset.py", line 89, in __init__
    self._read_files_and_tokenize()
  File "/data/user_data/mingkaid/deepscaler/verl/verl/utils/dataset/rl_dataset.py", line 100, in _read_files_and_tokenize
    dataframe = pd.read_parquet(parquet_file)
  File "/home/mingkaid/miniforge3/envs/deepscaler/lib/python3.10/site-packages/pandas/io/parquet.py", line 667, in read_parquet
    return impl.read(
  File "/home/mingkaid/miniforge3/envs/deepscaler/lib/python3.10/site-packages/pandas/io/parquet.py", line 267, in read
    path_or_handle, handles, filesystem = _get_path_or_handle(
  File "/home/mingkaid/miniforge3/envs/deepscaler/lib/python3.10/site-packages/pandas/io/parquet.py", line 140, in _get_path_or_handle
    handles = get_handle(
  File "/home/mingkaid/miniforge3/envs/deepscaler/lib/python3.10/site-packages/pandas/io/common.py", line 882, in get_handle
    handle = open(handle, ioargs.mode)
FileNotFoundError: [Errno 2] No such file or directory: '/home/mingkaid/deepscaler/data/train.parquet'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
